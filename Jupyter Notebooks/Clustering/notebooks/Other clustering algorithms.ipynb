{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [DBSCAN](#1.-DBSCAN)\n",
    "- [Agglomerative clustering](#2.-Agglomerative-clustering)\n",
    "- [Gaussian Mixtures](3.-Gaussian-Mixtures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density-based spatial clustering of applications with noise (DBSCAN) is a clustering algorithm that works well if all the clusters are dense enough and if they are well separated by low-density regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parameters to the algorithm, `min_samples` and `eps`, which define formally what we mean when we say dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN:\n",
    "    \n",
    "- $\\epsilon$-neighborhoods: For each data point, DBSCAN counts how many points are located within a distance `eps`.\n",
    "- Core data points: If a data point has at least `min_samples` points in its $\\epsilon$-neighborhood, then it is considered a **core point**.\n",
    "- Clusters: all points in the neighborhood of a core point belong to the same cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X,y = make_moons(n_samples = 1000,\n",
    "                 noise = 0.05)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.05,min_samples=5)\n",
    "dbscan.fit(X)\n",
    "# cluster labels (label = -1 means that the point is considered an anomaly)\n",
    "labels = dbscan.labels_\n",
    "# core points\n",
    "cores = dbscan.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "for i in np.unique(labels):\n",
    "    if i!=-1:\n",
    "        plt.scatter(X[labels==i,0],X[labels==i,1], label='cluster '+str(i))\n",
    "    else:\n",
    "        plt.scatter(X[labels==i,0],X[labels==i,1], label='anomaly')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.scatter(cores[:,0],cores[:,1],s=100,marker='x',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.2,min_samples=5)\n",
    "dbscan.fit(X)\n",
    "# cluster labels (label = -1 means that the point is considered an anomaly)\n",
    "labels = dbscan.labels_\n",
    "# core points\n",
    "cores = dbscan.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "for i in np.unique(labels):\n",
    "    if i!=-1:\n",
    "        plt.scatter(X[labels==i,0],X[labels==i,1], label='cluster '+str(i))\n",
    "    else:\n",
    "        plt.scatter(X[labels==i,0],X[labels==i,1], label='anomaly')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN pros and cons**:\n",
    "\n",
    "**Pros:**\n",
    "- does not require to specify number of clusters beforehand.\n",
    "- Performs well with arbitrary shapes clusters.\n",
    "- DBSCAN is robust to outliers and able to detect the outliers.\n",
    "\n",
    "**Cons:**\n",
    "- determining an appropriate distance of neighborhood (eps) is not easy and it requires domain knowledge.\n",
    "- it does not generalize well to clusters with much different densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hierarchy of clusters is built from the bottom up.\n",
    " Each data point is assumed to be a separate cluster at first. Then the similar clusters are iteratively combined.\n",
    " There are 4 different methods implemented in scikit-learn to measure the similarity:\n",
    "- Ward’s linkage: Minimizes the variance of the clusters being merged. Least increase in total variance around cluster centroids is aimed.\n",
    "- Average linkage: Average distance of each data point in two clusters.\n",
    "- Complete (maximum) linkage: Maximum distance among all data points in two clusters.\n",
    "- Single (minimum) linkage: Maximum distance among all data points in two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X,y = make_circles(n_samples=1000, noise=0.05, factor=0.5)\n",
    "plt.scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkage = ‘ward’, ‘complete’, ‘average’, or ‘single’}\n",
    "k = 2\n",
    "aggclt = AgglomerativeClustering(n_clusters = k, linkage='single')\n",
    "aggclt.fit(X)\n",
    "# cluster labels\n",
    "labels = aggclt.labels_\n",
    "# plot clusters\n",
    "for i in range(k):\n",
    "    plt.scatter(X[labels==i,0],X[labels==i,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/um-perez-alvaro/Data-Science-Practice/master/Data/iris.csv'\n",
    "iris = pd.read_csv(url)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop('species',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "aggclt = AgglomerativeClustering(n_clusters=k,linkage='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggclt.fit(X)\n",
    "labels = aggclt.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters = species?\n",
    "pd.crosstab(labels, iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below plots the corresponding dendrogram of a hierarchical\n",
    "# clustering using AgglomerativeClustering and the dendrogram method available in scipy.\n",
    "\n",
    "def plot_dendrogram(model):\n",
    "\n",
    "    from scipy.cluster.hierarchy import dendrogram\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggclt = AgglomerativeClustering(n_clusters=None, linkage='average', distance_threshold=0)\n",
    "aggclt.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plot_dendrogram(aggclt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It assumes that the cluster points were generated from a mixture of Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gm = GaussianMixture(n_components=3)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the resulting decision boundaries (dashed lines) and density contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_mixture(model, X):\n",
    "    from matplotlib.colors import LogNorm    \n",
    "    # feature names\n",
    "    try:\n",
    "        feature_names = X.columns\n",
    "    except:\n",
    "        feature_names = ['feature #1','feature #2']\n",
    "    \n",
    "    # put data into numpy arrays\n",
    "    try:\n",
    "        X = np.array(X)\n",
    "    except:\n",
    "        print('something went wrong')\n",
    "    \n",
    "    \n",
    "    resolution = 1000\n",
    "    \n",
    "    # create a mesh grid\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = -model.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z,\n",
    "                 norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                 levels=np.logspace(0, 2, 12))\n",
    "    plt.contour(xx, yy, Z,\n",
    "                norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                levels=np.logspace(0, 2, 12),\n",
    "                linewidths=1, colors='k')\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z,\n",
    "                linewidths=2, colors='r', linestyles='dashed')\n",
    "    \n",
    "    # plot data\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "    \n",
    "    # plot centroids\n",
    "    centroids = model.means_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=20, linewidths=8,\n",
    "                color='w', zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=50, linewidths=2,\n",
    "                color='r', zorder=11, alpha=1)\n",
    "\n",
    "    plt.xlabel(feature_names[0], fontsize=15)\n",
    "    plt.ylabel(feature_names[1], fontsize=15)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gaussian_mixture(gm, X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
